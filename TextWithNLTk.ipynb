{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire process of making text noise free and ready for analysis is called Text Processing.\n",
    "\n",
    "    It consists of three steps.\n",
    "        (1). Noise Removal\n",
    "        (2). Lexicon Normalization.\n",
    "        (3). Object Standarization.\n",
    "        \n",
    "  Raw Text ->\n",
    "  \n",
    "  NoiseRemoval([Stop Words, URLs, puntuations ..]) -> \n",
    "  \n",
    "  Word Normalization([Tokenization, Lemmatization, Stemming]) -> \n",
    "  \n",
    "  WordStandarization([Regular Expression, Lookup Tables]) -> \n",
    "  \n",
    "  Cleaned Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Noise - Any thing that is not valuable for given context is called Noise.\n",
    "\n",
    "Examples of Noise:-\n",
    "\n",
    "    (1). Language Stop Words.\n",
    "    (2). URLs, Media Link,\n",
    "    (3). Industry Specific words.\n",
    "\n",
    "General approach for removing noise is create a Dictionary of noise entity(word) and iterate through the text to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample code to remove the noisy words from text\n",
    "\n",
    "noise_entity = set(['a', 'an', 'the', 'that', 'This', 'is'])\n",
    "\n",
    "def _remove_noise(text):\n",
    "    text_list = text.split()\n",
    "    cleaned_list = [item for item in text_list if item not in noise_entity]\n",
    "    cleaned_text = ' '.join(cleaned_list)\n",
    "    return cleaned_text\n",
    "_remove_noise(\"This is a text\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from analytics vidhya'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regular expression can also be used to filter some kind of noise\n",
    "import re\n",
    "def _remove_noise_regex(text, pattern):\n",
    "    urls = re.finditer(pattern, text)\n",
    "    \n",
    "    for i in urls:\n",
    "        text = re.sub(i.group().strip(), '', text)\n",
    "    return text\n",
    "\n",
    "regex_pattern = '#[\\w]*'\n",
    "_remove_noise_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Normalization\n",
    "    (1). Stemming - Stemming is a rudimentary rule-based process to remove suffixes like (s, es, ly)\n",
    "    (2). Lemmatization - Organized way of finding root of the word. \n",
    "        It considers word structure and grammer relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word='playing'\n",
    "print(lem.lemmatize(word, 'v'))\n",
    "\n",
    "print(stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Standarization - Words and Phrases that are not present in any standard lexical dictionaries.\n",
    "Some examples are acronyms, hashtags, colloquial Slangs.\n",
    "\n",
    "Examples - awsm, rt, dm, luv\n",
    "\n",
    "With help of regular expresion and prepared data dictionaries these cases can be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is retweeted by me saying NLP is awesome'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'awsm': 'awesome', 'luv': 'Love', 'rt': 'Retweet'}\n",
    "\n",
    "def _lookup_words(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            temp = lookup_dict[word.lower()]\n",
    "            new_words.append(temp)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "_lookup_words(\"RT this is retweeted by me saying NLP is awsm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text to Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depending on use text can be converted into features by assorted techniques\n",
    "    1. Syntactical Parsing - Analysis of words in sentence as per grammar. \n",
    "        Dependency Trees and POS tags are important for text syntactics.\n",
    "    \n",
    "    2. N-gram, Word-based features - \n",
    "    3. Staistical Features\n",
    "    4. Word Embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'VBG'), ('from', 'IN'), ('various', 'JJ'), ('sources', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"I am learning Natural Language Processing from various sources\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pos tagging is useful in many NLP purposes.\n",
    "    1. Word sense disambiguation  - Please book my flight. I am reading this book in flight\n",
    "    2. Improving word-based features - Pos tagging helps to preserve the context, thus make strong feature.\n",
    "    3. Normalization & lemmatization - Pos tags are basis of Lemmatization.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction\n",
    "\n",
    "Entities are the most important chunks of a sentence. \n",
    "\n",
    "Entitiy detection algorithms are generally ensemle of rule based, dictonary lookups, pos-tagging, and dependency parsing.\n",
    "\n",
    "Tha applicabilty can be seen as automated chatbots, content analyzers and consumer-insights.\n",
    "                  \n",
    "  \n",
    "                  \n",
    "***At the W party Thursday{__day__} night{__time__} at Nandi-Hills{Place}.***\n",
    "\n",
    "\n",
    "\n",
    "Topic modelling & Named Entity Recognition are two key entity detection methods in NLP.\n",
    "\n",
    "__1. Named Entity Recognition : __\n",
    "Sentence - Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n",
    "\n",
    "Named Entity - (\"Sergey Brin\", Person), (\"Google Inc\"- Org), (\"New York\"- Place)\n",
    "\n",
    "A typical named entity consists three blocks.\n",
    "\n",
    "_Noun Phrase Identification_ - Extracting noun phrases from a text using POS tagging or dependency parsing.\n",
    "\n",
    "__2. Topic Modeling__\n",
    "\n",
    "Topic modeling is process of identifying the topics present in text corpus in unsupervised manner.\n",
    "\n",
    "Topics are defined as \"a repeating pattern of co-occuring terms in corpus.\"\n",
    "\n",
    "A good topic modeler will result:   \"Health\", \"Doctor\", \"Patient\", \"hospital\" for topic - _Healthcare_. and\n",
    "\n",
    "\"Farms\", \"Wheat\", \"Corps\" for topic _Farming_\n",
    "\n",
    "**LDA(Latent Dirichlet Allocation)** is most popular topic modeling technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "\n",
    "doc = [doc1, doc2, doc3]\n",
    "doc_clean = [d.split() for d in doc]\n",
    "\n",
    "import gensim\n",
    "import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram Featres\n",
    "\n",
    "A combination of N words together are called N-Gram. N=2 is considered most important features of all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', 'Name'], ['Name', 'is'], ['is', 'Anshu'], ['Anshu', 'Kumar']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def n_gram(string, n):\n",
    "    string_list = string.split()\n",
    "    n_gram_list = []\n",
    "    i = 0\n",
    "    while i < len(string_list):\n",
    "        n_gram_list.append(string_list[i: i+n])\n",
    "        i+=n-1\n",
    "    return n_gram_list[:-1]\n",
    "n_gram(\"My Name is Anshu Kumar\", 2)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Statistical Features\n",
    "\n",
    "__TF-IDF__ TF-IDF converts text document to vectors on basis of occurence of words in document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = tfidf.fit_transform(corpus)\n",
    "#print(X.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 0 1]\n",
      " [1 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(CountVectorizer().fit_transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.58448290102\n",
      "  (0, 2)\t0.58448290102\n",
      "  (0, 4)\t0.444514311537\n",
      "  (0, 1)\t0.345205016865\n",
      "  (1, 1)\t0.385371627466\n",
      "  (1, 0)\t0.652490884513\n",
      "  (1, 3)\t0.652490884513\n",
      "  (2, 4)\t0.444514311537\n",
      "  (2, 1)\t0.345205016865\n",
      "  (2, 6)\t0.58448290102\n",
      "  (2, 5)\t0.58448290102\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another', 'document', 'is', 'random', 'sample', 'text', 'third', 'this']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "This is modern way of representing text as vector of real numbers. Word embedding represents a word in fixed \n",
    "dimension.\n",
    "\n",
    "A word \"man\" might be represented in 5-d, [4.2, 4.5, 1.1, 3.76, 1.9], each values represents the magnitude in particular dimension.\n",
    "\n",
    "Aim of word embedding is to redefine the high dimensional word features into low-dimensional feature vectors\n",
    "by preserving the contextual similarty in corpus.\n",
    "\n",
    "They are widely used in CNNs and RNNs.\n",
    "\n",
    "Word2Vec and glove are two popular models to create word embeddings of text.\n",
    "\n",
    "Word2Vec model is composed of preprocessing shallow neural network modules, \n",
    "    1. Continuous Bagof words\n",
    "    2. Skip-Gram\n",
    "Word2Vec first constructs a vocabulary from given corpus and then learn word embedding representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model similartity:  -0.0208757325844\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print('model similartity: ', model.similarity('data', 'learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This similarity scores can be used to measure text similarity using cosine similarity techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
